<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.3 Principal Components | An Introduction to Spatial Data Science with GeoDa</title>
  <meta name="description" content="GeoDa" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="2.3 Principal Components | An Introduction to Spatial Data Science with GeoDa" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="GeoDa" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.3 Principal Components | An Introduction to Spatial Data Science with GeoDa" />
  
  <meta name="twitter:description" content="GeoDa" />
  

<meta name="author" content="Luc Anselin" />


<meta name="date" content="2023-08-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="matrix-algebra-review.html"/>
<link rel="next" href="vizpca.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">An Introduction to Spatial Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview-of-volume-2.html"><a href="overview-of-volume-2.html"><i class="fa fa-check"></i><b>1.1</b> Overview of Volume 2</a></li>
<li class="chapter" data-level="1.2" data-path="sample-data-sets.html"><a href="sample-data-sets.html"><i class="fa fa-check"></i><b>1.2</b> Sample Data Sets</a></li>
</ul></li>
<li class="part"><span><b>I Dimension Reduction</b></span></li>
<li class="chapter" data-level="2" data-path="CHPCA.html"><a href="CHPCA.html"><i class="fa fa-check"></i><b>2</b> Principal Component Analysis (PCA)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="topics-covered.html"><a href="topics-covered.html"><i class="fa fa-check"></i><b>2.1</b> Topics Covered</a></li>
<li class="chapter" data-level="2.2" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html"><i class="fa fa-check"></i><b>2.2</b> Matrix Algebra Review</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="2.2.2" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#matrix-decompositions"><i class="fa fa-check"></i><b>2.2.2</b> Matrix decompositions</a>
<ul>
<li class="chapter" data-level="2.2.2.1" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#spectraldecomposition"><i class="fa fa-check"></i><b>2.2.2.1</b> Spectral decomposition</a></li>
<li class="chapter" data-level="2.2.2.2" data-path="matrix-algebra-review.html"><a href="matrix-algebra-review.html#svd"><i class="fa fa-check"></i><b>2.2.2.2</b> Singular value decomposition (SVD)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="principal-components.html"><a href="principal-components.html"><i class="fa fa-check"></i><b>2.3</b> Principal Components</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="principal-components.html"><a href="principal-components.html#implementation"><i class="fa fa-check"></i><b>2.3.1</b> Implementation</a>
<ul>
<li class="chapter" data-level="2.3.1.1" data-path="principal-components.html"><a href="principal-components.html#saving-the-principal-components"><i class="fa fa-check"></i><b>2.3.1.1</b> Saving the principal components</a></li>
<li class="chapter" data-level="2.3.1.2" data-path="principal-components.html"><a href="principal-components.html#saving-the-result-summary"><i class="fa fa-check"></i><b>2.3.1.2</b> Saving the result summary</a></li>
</ul></li>
<li class="chapter" data-level="2.3.2" data-path="principal-components.html"><a href="principal-components.html#pcainterpretation"><i class="fa fa-check"></i><b>2.3.2</b> Interpretation</a>
<ul>
<li class="chapter" data-level="2.3.2.1" data-path="principal-components.html"><a href="principal-components.html#pcaexplainedvar"><i class="fa fa-check"></i><b>2.3.2.1</b> Explained variance</a></li>
<li class="chapter" data-level="2.3.2.2" data-path="principal-components.html"><a href="principal-components.html#variable-loadings"><i class="fa fa-check"></i><b>2.3.2.2</b> Variable loadings</a></li>
<li class="chapter" data-level="2.3.2.3" data-path="principal-components.html"><a href="principal-components.html#loadingsandpca"><i class="fa fa-check"></i><b>2.3.2.3</b> Variable loadings and principal components</a></li>
<li class="chapter" data-level="2.3.2.4" data-path="principal-components.html"><a href="principal-components.html#substantive-interpretation---squared-correlation"><i class="fa fa-check"></i><b>2.3.2.4</b> Substantive interpretation - squared correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="vizpca.html"><a href="vizpca.html"><i class="fa fa-check"></i><b>2.4</b> Visualizing principal components</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="vizpca.html"><a href="vizpca.html#scatter-plot"><i class="fa fa-check"></i><b>2.4.1</b> Scatter plot</a></li>
<li class="chapter" data-level="2.4.2" data-path="vizpca.html"><a href="vizpca.html#multivariate-decomposition"><i class="fa fa-check"></i><b>2.4.2</b> Multivariate decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="spatializing-principal-components.html"><a href="spatializing-principal-components.html"><i class="fa fa-check"></i><b>2.5</b> Spatializing Principal Components</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="spatializing-principal-components.html"><a href="spatializing-principal-components.html#principal-component-map"><i class="fa fa-check"></i><b>2.5.1</b> Principal component map</a></li>
<li class="chapter" data-level="2.5.2" data-path="spatializing-principal-components.html"><a href="spatializing-principal-components.html#univariate-cluster-map"><i class="fa fa-check"></i><b>2.5.2</b> Univariate cluster map</a></li>
<li class="chapter" data-level="2.5.3" data-path="spatializing-principal-components.html"><a href="spatializing-principal-components.html#principal-components-as-multivariate-cluster-maps"><i class="fa fa-check"></i><b>2.5.3</b> Principal components as multivariate cluster maps</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="CHMDS.html"><a href="CHMDS.html"><i class="fa fa-check"></i><b>3</b> Multidimensional Scaling (MDS)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="topics-covered-1.html"><a href="topics-covered-1.html"><i class="fa fa-check"></i><b>3.1</b> Topics Covered</a></li>
<li class="chapter" data-level="3.2" data-path="classic-metric-scaling.html"><a href="classic-metric-scaling.html"><i class="fa fa-check"></i><b>3.2</b> Classic Metric Scaling</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classic-metric-scaling.html"><a href="classic-metric-scaling.html#mathematical-details"><i class="fa fa-check"></i><b>3.2.1</b> Mathematical Details</a>
<ul>
<li class="chapter" data-level="3.2.1.1" data-path="classic-metric-scaling.html"><a href="classic-metric-scaling.html#classic-metric-mds-and-principal-components"><i class="fa fa-check"></i><b>3.2.1.1</b> Classic metric MDS and principal components</a></li>
<li class="chapter" data-level="3.2.1.2" data-path="classic-metric-scaling.html"><a href="classic-metric-scaling.html#poweriteration"><i class="fa fa-check"></i><b>3.2.1.2</b> Power iteration method</a></li>
<li class="chapter" data-level="3.2.1.3" data-path="classic-metric-scaling.html"><a href="classic-metric-scaling.html#Gramanddissimilarity"><i class="fa fa-check"></i><b>3.2.1.3</b> Dissimilarity matrix</a></li>
</ul></li>
<li class="chapter" data-level="3.2.2" data-path="classic-metric-scaling.html"><a href="classic-metric-scaling.html#MDSimplementation"><i class="fa fa-check"></i><b>3.2.2</b> Implementation</a>
<ul>
<li class="chapter" data-level="3.2.2.1" data-path="classic-metric-scaling.html"><a href="classic-metric-scaling.html#saving-the-mds-coordinates"><i class="fa fa-check"></i><b>3.2.2.1</b> Saving the MDS coordinates</a></li>
<li class="chapter" data-level="3.2.2.2" data-path="classic-metric-scaling.html"><a href="classic-metric-scaling.html#mds-and-pca"><i class="fa fa-check"></i><b>3.2.2.2</b> MDS and PCA</a></li>
<li class="chapter" data-level="3.2.2.3" data-path="classic-metric-scaling.html"><a href="classic-metric-scaling.html#power-approximation"><i class="fa fa-check"></i><b>3.2.2.3</b> Power approximation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="smacof.html"><a href="smacof.html"><i class="fa fa-check"></i><b>3.3</b> SMACOF</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="smacof.html"><a href="smacof.html#mathematical-details-1"><i class="fa fa-check"></i><b>3.3.1</b> Mathematical Details</a></li>
<li class="chapter" data-level="3.3.2" data-path="smacof.html"><a href="smacof.html#implementation-1"><i class="fa fa-check"></i><b>3.3.2</b> Implementation</a>
<ul>
<li class="chapter" data-level="3.3.2.1" data-path="smacof.html"><a href="smacof.html#manhattan-block-distance"><i class="fa fa-check"></i><b>3.3.2.1</b> Manhattan block distance</a></li>
<li class="chapter" data-level="3.3.2.2" data-path="smacof.html"><a href="smacof.html#smacofvsclassic"><i class="fa fa-check"></i><b>3.3.2.2</b> SMACOF vs classic metric MDS</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="vizmds.html"><a href="vizmds.html"><i class="fa fa-check"></i><b>3.4</b> Visualizing MDS</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="vizmds.html"><a href="vizmds.html#mds-and-parallel-coordinate-plot"><i class="fa fa-check"></i><b>3.4.1</b> MDS and Parallel Coordinate Plot</a></li>
<li class="chapter" data-level="3.4.2" data-path="vizmds.html"><a href="vizmds.html#MDScategories"><i class="fa fa-check"></i><b>3.4.2</b> MDS Scatter Plot with Categories</a></li>
<li class="chapter" data-level="3.4.3" data-path="vizmds.html"><a href="vizmds.html#d-mds"><i class="fa fa-check"></i><b>3.4.3</b> 3-D MDS</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="spatializemds.html"><a href="spatializemds.html"><i class="fa fa-check"></i><b>3.5</b> Spatializing MDS</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="spatializemds.html"><a href="spatializemds.html#mds-and-map"><i class="fa fa-check"></i><b>3.5.1</b> MDS and Map</a></li>
<li class="chapter" data-level="3.5.2" data-path="spatializemds.html"><a href="spatializemds.html#mds-spatial-weights"><i class="fa fa-check"></i><b>3.5.2</b> MDS Spatial Weights</a>
<ul>
<li class="chapter" data-level="3.5.2.1" data-path="spatializemds.html"><a href="spatializemds.html#attribute-and-geographical-neighbors"><i class="fa fa-check"></i><b>3.5.2.1</b> Attribute and geographical neighbors</a></li>
<li class="chapter" data-level="3.5.2.2" data-path="spatializemds.html"><a href="spatializemds.html#MDSneighborsimilarity"><i class="fa fa-check"></i><b>3.5.2.2</b> Common coverage percentage</a></li>
</ul></li>
<li class="chapter" data-level="3.5.3" data-path="spatializemds.html"><a href="spatializemds.html#mdsneighbormatch"><i class="fa fa-check"></i><b>3.5.3</b> MDS Neighbor Match Test</a></li>
<li class="chapter" data-level="3.5.4" data-path="spatializemds.html"><a href="spatializemds.html#hdbscan-and-mds"><i class="fa fa-check"></i><b>3.5.4</b> HDBSCAN and MDS</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CHSNE.html"><a href="CHSNE.html"><i class="fa fa-check"></i><b>4</b> Stochastic Neighbor Embedding (SNE)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="topics-covered-2.html"><a href="topics-covered-2.html"><i class="fa fa-check"></i><b>4.1</b> Topics Covered</a></li>
<li class="chapter" data-level="4.2" data-path="basics-of-information-theory.html"><a href="basics-of-information-theory.html"><i class="fa fa-check"></i><b>4.2</b> Basics of Information Theory</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="basics-of-information-theory.html"><a href="basics-of-information-theory.html#stochastic-neighbors"><i class="fa fa-check"></i><b>4.2.1</b> Stochastic Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="t-sne.html"><a href="t-sne.html"><i class="fa fa-check"></i><b>4.3</b> t-SNE</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="t-sne.html"><a href="t-sne.html#cost-function-and-optimization"><i class="fa fa-check"></i><b>4.3.1</b> Cost Function and Optimization</a></li>
<li class="chapter" data-level="4.3.2" data-path="t-sne.html"><a href="t-sne.html#large-data-applications-barnes-hut"><i class="fa fa-check"></i><b>4.3.2</b> Large Data Applications (Barnes-Hut)</a>
<ul>
<li class="chapter" data-level="4.3.2.1" data-path="t-sne.html"><a href="t-sne.html#simplification-of-p"><i class="fa fa-check"></i><b>4.3.2.1</b> Simplification of <span class="math inline">\(P\)</span></a></li>
<li class="chapter" data-level="4.3.2.2" data-path="t-sne.html"><a href="t-sne.html#BarnesHutQ"><i class="fa fa-check"></i><b>4.3.2.2</b> Simplification of <span class="math inline">\(Q\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="implementation-2.html"><a href="implementation-2.html"><i class="fa fa-check"></i><b>4.4</b> Implementation</a>
<ul>
<li class="chapter" data-level="4.4.0.1" data-path="implementation-2.html"><a href="implementation-2.html#inspecting-the-iterations"><i class="fa fa-check"></i><b>4.4.0.1</b> Inspecting the iterations</a></li>
<li class="chapter" data-level="4.4.1" data-path="implementation-2.html"><a href="implementation-2.html#tsneanimation"><i class="fa fa-check"></i><b>4.4.1</b> Animation</a></li>
<li class="chapter" data-level="4.4.2" data-path="implementation-2.html"><a href="implementation-2.html#tuning-the-optimization"><i class="fa fa-check"></i><b>4.4.2</b> Tuning the Optimization</a>
<ul>
<li class="chapter" data-level="4.4.2.1" data-path="implementation-2.html"><a href="implementation-2.html#theta"><i class="fa fa-check"></i><b>4.4.2.1</b> Theta</a></li>
<li class="chapter" data-level="4.4.2.2" data-path="implementation-2.html"><a href="implementation-2.html#perplexity"><i class="fa fa-check"></i><b>4.4.2.2</b> Perplexity</a></li>
<li class="chapter" data-level="4.4.2.3" data-path="implementation-2.html"><a href="implementation-2.html#iteration-momentum-switch"><i class="fa fa-check"></i><b>4.4.2.3</b> Iteration momentum switch</a></li>
</ul></li>
<li class="chapter" data-level="4.4.3" data-path="implementation-2.html"><a href="implementation-2.html#interpretation-and-spatialization"><i class="fa fa-check"></i><b>4.4.3</b> Interpretation and Spatialization</a>
<ul>
<li class="chapter" data-level="4.4.3.1" data-path="implementation-2.html"><a href="implementation-2.html#nearest-neighbor-match-test"><i class="fa fa-check"></i><b>4.4.3.1</b> Nearest neighbor match test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="comparing-distance-preserving-methods.html"><a href="comparing-distance-preserving-methods.html"><i class="fa fa-check"></i><b>4.5</b> Comparing Distance Preserving Methods</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="comparing-distance-preserving-methods.html"><a href="comparing-distance-preserving-methods.html#comparing-t-sne-options"><i class="fa fa-check"></i><b>4.5.1</b> Comparing t-SNE Options</a></li>
<li class="chapter" data-level="4.5.2" data-path="comparing-distance-preserving-methods.html"><a href="comparing-distance-preserving-methods.html#local-fit-with-common-coverage-percentage"><i class="fa fa-check"></i><b>4.5.2</b> Local Fit with Common Coverage Percentage</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Classic Clustering</b></span></li>
<li class="chapter" data-level="5" data-path="CHhierarchicalclustering.html"><a href="CHhierarchicalclustering.html"><i class="fa fa-check"></i><b>5</b> Hierarchical Clustering Methods</a>
<ul>
<li class="chapter" data-level="5.1" data-path="topics-covered-3.html"><a href="topics-covered-3.html"><i class="fa fa-check"></i><b>5.1</b> Topics Covered</a></li>
<li class="chapter" data-level="5.2" data-path="dissimilarity.html"><a href="dissimilarity.html"><i class="fa fa-check"></i><b>5.2</b> Dissimilarity</a></li>
<li class="chapter" data-level="5.3" data-path="agglomerative-clustering.html"><a href="agglomerative-clustering.html"><i class="fa fa-check"></i><b>5.3</b> Agglomerative Clustering</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="agglomerative-clustering.html"><a href="agglomerative-clustering.html#linkage-and-updating-formula"><i class="fa fa-check"></i><b>5.3.1</b> Linkage and Updating Formula</a>
<ul>
<li class="chapter" data-level="5.3.1.1" data-path="agglomerative-clustering.html"><a href="agglomerative-clustering.html#single-linkage"><i class="fa fa-check"></i><b>5.3.1.1</b> Single linkage</a></li>
<li class="chapter" data-level="5.3.1.2" data-path="agglomerative-clustering.html"><a href="agglomerative-clustering.html#complete-linkage"><i class="fa fa-check"></i><b>5.3.1.2</b> Complete linkage</a></li>
<li class="chapter" data-level="5.3.1.3" data-path="agglomerative-clustering.html"><a href="agglomerative-clustering.html#average-linkage"><i class="fa fa-check"></i><b>5.3.1.3</b> Average linkage</a></li>
<li class="chapter" data-level="5.3.1.4" data-path="agglomerative-clustering.html"><a href="agglomerative-clustering.html#wards-method"><i class="fa fa-check"></i><b>5.3.1.4</b> Ward’s method</a></li>
<li class="chapter" data-level="5.3.1.5" data-path="agglomerative-clustering.html"><a href="agglomerative-clustering.html#illustration---single-linkage"><i class="fa fa-check"></i><b>5.3.1.5</b> Illustration - single linkage</a></li>
</ul></li>
<li class="chapter" data-level="5.3.2" data-path="agglomerative-clustering.html"><a href="agglomerative-clustering.html#dendrogram"><i class="fa fa-check"></i><b>5.3.2</b> Dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="implementation-3.html"><a href="implementation-3.html"><i class="fa fa-check"></i><b>5.4</b> Implementation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="implementation-3.html"><a href="implementation-3.html#hierarchicalvariables"><i class="fa fa-check"></i><b>5.4.1</b> Variable Settings Dialog</a></li>
<li class="chapter" data-level="5.4.2" data-path="implementation-3.html"><a href="implementation-3.html#wards-method-1"><i class="fa fa-check"></i><b>5.4.2</b> Ward’s method</a></li>
<li class="chapter" data-level="5.4.3" data-path="implementation-3.html"><a href="implementation-3.html#single-linkage-1"><i class="fa fa-check"></i><b>5.4.3</b> Single linkage</a></li>
<li class="chapter" data-level="5.4.4" data-path="implementation-3.html"><a href="implementation-3.html#complete-linkage-1"><i class="fa fa-check"></i><b>5.4.4</b> Complete linkage</a></li>
<li class="chapter" data-level="5.4.5" data-path="implementation-3.html"><a href="implementation-3.html#average-linkage-1"><i class="fa fa-check"></i><b>5.4.5</b> Average linkage</a></li>
<li class="chapter" data-level="5.4.6" data-path="implementation-3.html"><a href="implementation-3.html#sensitivity-analysis"><i class="fa fa-check"></i><b>5.4.6</b> Sensitivity Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="CHPartioningCluster.html"><a href="CHPartioningCluster.html"><i class="fa fa-check"></i><b>6</b> Partioning Clustering Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="topics-covered-4.html"><a href="topics-covered-4.html"><i class="fa fa-check"></i><b>6.1</b> Topics Covered</a></li>
<li class="chapter" data-level="6.2" data-path="the-k-means-algorithm.html"><a href="the-k-means-algorithm.html"><i class="fa fa-check"></i><b>6.2</b> The K Means Algorithm</a>
<ul>
<li class="chapter" data-level="" data-path="the-k-means-algorithm.html"><a href="the-k-means-algorithm.html#mathematical-details-2"><i class="fa fa-check"></i>Mathematical details</a></li>
<li class="chapter" data-level="6.2.1" data-path="the-k-means-algorithm.html"><a href="the-k-means-algorithm.html#iterativerelocation"><i class="fa fa-check"></i><b>6.2.1</b> Iterative Relocation</a></li>
<li class="chapter" data-level="6.2.2" data-path="the-k-means-algorithm.html"><a href="the-k-means-algorithm.html#the-choice-of-k"><i class="fa fa-check"></i><b>6.2.2</b> The Choice of K</a></li>
<li class="chapter" data-level="6.2.3" data-path="the-k-means-algorithm.html"><a href="the-k-means-algorithm.html#kmeansplusplus"><i class="fa fa-check"></i><b>6.2.3</b> K-means++</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="implementation-4.html"><a href="implementation-4.html"><i class="fa fa-check"></i><b>6.3</b> Implementation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="implementation-4.html"><a href="implementation-4.html#digressionpca"><i class="fa fa-check"></i><b>6.3.1</b> Digression: Clustering with Dimension Reduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="implementation-4.html"><a href="implementation-4.html#cluster-parameters"><i class="fa fa-check"></i><b>6.3.2</b> Cluster Parameters</a></li>
<li class="chapter" data-level="6.3.3" data-path="implementation-4.html"><a href="implementation-4.html#cluster-results"><i class="fa fa-check"></i><b>6.3.3</b> Cluster Results</a>
<ul>
<li class="chapter" data-level="6.3.3.1" data-path="implementation-4.html"><a href="implementation-4.html#adjustclusterlabels"><i class="fa fa-check"></i><b>6.3.3.1</b> Adjusting cluster labels</a></li>
</ul></li>
<li class="chapter" data-level="6.3.4" data-path="implementation-4.html"><a href="implementation-4.html#kmeansoptions"><i class="fa fa-check"></i><b>6.3.4</b> Options and Sensitivity Analysis</a>
<ul>
<li class="chapter" data-level="6.3.4.1" data-path="implementation-4.html"><a href="implementation-4.html#initialization"><i class="fa fa-check"></i><b>6.3.4.1</b> Initialization</a></li>
<li class="chapter" data-level="6.3.4.2" data-path="implementation-4.html"><a href="implementation-4.html#kmeanselbowplot"><i class="fa fa-check"></i><b>6.3.4.2</b> Selecting k – Elbow plot</a></li>
<li class="chapter" data-level="6.3.4.3" data-path="implementation-4.html"><a href="implementation-4.html#standardization"><i class="fa fa-check"></i><b>6.3.4.3</b> Standardization</a></li>
<li class="chapter" data-level="6.3.4.4" data-path="implementation-4.html"><a href="implementation-4.html#kmeansminbound"><i class="fa fa-check"></i><b>6.3.4.4</b> Minimum bound</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="cluster-categories-as-variables.html"><a href="cluster-categories-as-variables.html"><i class="fa fa-check"></i><b>6.4</b> Cluster Categories as Variables</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="cluster-categories-as-variables.html"><a href="cluster-categories-as-variables.html#conditional-box-plot"><i class="fa fa-check"></i><b>6.4.1</b> Conditional Box Plot</a></li>
<li class="chapter" data-level="6.4.2" data-path="cluster-categories-as-variables.html"><a href="cluster-categories-as-variables.html#clusteraggregation"><i class="fa fa-check"></i><b>6.4.2</b> Aggregation by Cluster</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="CHAdvancedClustering.html"><a href="CHAdvancedClustering.html"><i class="fa fa-check"></i><b>7</b> Advanced Clustering Methods</a>
<ul>
<li class="chapter" data-level="7.1" data-path="topics-covered-5.html"><a href="topics-covered-5.html"><i class="fa fa-check"></i><b>7.1</b> Topics Covered</a></li>
<li class="chapter" data-level="7.2" data-path="k-medians.html"><a href="k-medians.html"><i class="fa fa-check"></i><b>7.2</b> K-Medians</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="k-medians.html"><a href="k-medians.html#implementation-5"><i class="fa fa-check"></i><b>7.2.1</b> Implementation</a></li>
<li class="chapter" data-level="7.2.2" data-path="k-medians.html"><a href="k-medians.html#options-and-sensitivity-analysis"><i class="fa fa-check"></i><b>7.2.2</b> Options and Sensitivity Analysis</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="kmedoids.html"><a href="kmedoids.html"><i class="fa fa-check"></i><b>7.3</b> K-Medoids</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="kmedoids.html"><a href="kmedoids.html#the-pam-algorithm-for-k-medoids"><i class="fa fa-check"></i><b>7.3.1</b> The PAM Algorithm for K-Medoids</a>
<ul>
<li class="chapter" data-level="7.3.1.1" data-path="kmedoids.html"><a href="kmedoids.html#build"><i class="fa fa-check"></i><b>7.3.1.1</b> Build</a></li>
<li class="chapter" data-level="7.3.1.2" data-path="kmedoids.html"><a href="kmedoids.html#swap"><i class="fa fa-check"></i><b>7.3.1.2</b> Swap</a></li>
</ul></li>
<li class="chapter" data-level="7.3.2" data-path="kmedoids.html"><a href="kmedoids.html#improving-on-the-pam-algorithm"><i class="fa fa-check"></i><b>7.3.2</b> Improving on the PAM Algorithm</a>
<ul>
<li class="chapter" data-level="7.3.2.1" data-path="kmedoids.html"><a href="kmedoids.html#clara"><i class="fa fa-check"></i><b>7.3.2.1</b> CLARA</a></li>
<li class="chapter" data-level="7.3.2.2" data-path="kmedoids.html"><a href="kmedoids.html#clarans"><i class="fa fa-check"></i><b>7.3.2.2</b> CLARANS</a></li>
<li class="chapter" data-level="7.3.2.3" data-path="kmedoids.html"><a href="kmedoids.html#lab"><i class="fa fa-check"></i><b>7.3.2.3</b> LAB</a></li>
</ul></li>
<li class="chapter" data-level="7.3.3" data-path="kmedoids.html"><a href="kmedoids.html#kmedoidsimplementation"><i class="fa fa-check"></i><b>7.3.3</b> Implementation</a>
<ul>
<li class="chapter" data-level="7.3.3.1" data-path="kmedoids.html"><a href="kmedoids.html#cluster-results-1"><i class="fa fa-check"></i><b>7.3.3.1</b> Cluster results</a></li>
</ul></li>
<li class="chapter" data-level="7.3.4" data-path="kmedoids.html"><a href="kmedoids.html#options-and-sensitivity-analysis-1"><i class="fa fa-check"></i><b>7.3.4</b> Options and Sensitivity Analysis</a>
<ul>
<li class="chapter" data-level="7.3.4.1" data-path="kmedoids.html"><a href="kmedoids.html#clara-parameters"><i class="fa fa-check"></i><b>7.3.4.1</b> CLARA parameters</a></li>
<li class="chapter" data-level="7.3.4.2" data-path="kmedoids.html"><a href="kmedoids.html#clarans-parameters"><i class="fa fa-check"></i><b>7.3.4.2</b> CLARANS parameters</a></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="CHSpectralClustering.html"><a href="CHSpectralClustering.html"><i class="fa fa-check"></i><b>8</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="8.1" data-path="topics-covered-6.html"><a href="topics-covered-6.html"><i class="fa fa-check"></i><b>8.1</b> Topics Covered</a></li>
<li class="chapter" data-level="8.2" data-path="spectral-clustering-logic.html"><a href="spectral-clustering-logic.html"><i class="fa fa-check"></i><b>8.2</b> Spectral Clustering Logic</a></li>
<li class="chapter" data-level="8.3" data-path="clustering-as-a-graph-partitioning-problem.html"><a href="clustering-as-a-graph-partitioning-problem.html"><i class="fa fa-check"></i><b>8.3</b> Clustering as a Graph Partitioning Problem</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="clustering-as-a-graph-partitioning-problem.html"><a href="clustering-as-a-graph-partitioning-problem.html#graph-laplacian"><i class="fa fa-check"></i><b>8.3.1</b> Graph Laplacian</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="the-spectral-clustering-algorithm.html"><a href="the-spectral-clustering-algorithm.html"><i class="fa fa-check"></i><b>8.4</b> The Spectral Clustering Algorithm</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="the-spectral-clustering-algorithm.html"><a href="the-spectral-clustering-algorithm.html#spectraladjacency"><i class="fa fa-check"></i><b>8.4.1</b> Adjacency matrix</a></li>
<li class="chapter" data-level="8.4.2" data-path="the-spectral-clustering-algorithm.html"><a href="the-spectral-clustering-algorithm.html#clustering-on-the-eigenvectors-of-the-graph-laplacian"><i class="fa fa-check"></i><b>8.4.2</b> Clustering on the Eigenvectors of the Graph Laplacian</a></li>
<li class="chapter" data-level="8.4.3" data-path="the-spectral-clustering-algorithm.html"><a href="the-spectral-clustering-algorithm.html#spectralparameters"><i class="fa fa-check"></i><b>8.4.3</b> Spectral Clustering Parameters</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="implementation-6.html"><a href="implementation-6.html"><i class="fa fa-check"></i><b>8.5</b> Implementation</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="implementation-6.html"><a href="implementation-6.html#cluster-results-2"><i class="fa fa-check"></i><b>8.5.1</b> Cluster results</a></li>
<li class="chapter" data-level="8.5.2" data-path="implementation-6.html"><a href="implementation-6.html#options-and-sensitivity-analysis-2"><i class="fa fa-check"></i><b>8.5.2</b> Options and Sensitivity Analysis</a>
<ul>
<li class="chapter" data-level="8.5.2.1" data-path="implementation-6.html"><a href="implementation-6.html#k-nearest-neighbor-affinity-matrix"><i class="fa fa-check"></i><b>8.5.2.1</b> K-nearest neighbor affinity matrix</a></li>
<li class="chapter" data-level="8.5.2.2" data-path="implementation-6.html"><a href="implementation-6.html#gaussian-kernel-affinity-matrix"><i class="fa fa-check"></i><b>8.5.2.2</b> Gaussian kernel affinity matrix</a></li>
</ul></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Spatial Clustering</b></span></li>
<li class="chapter" data-level="9" data-path="CHspatialclassicclustering.html"><a href="CHspatialclassicclustering.html"><i class="fa fa-check"></i><b>9</b> Spatializing Classic Clustering Methods</a>
<ul>
<li class="chapter" data-level="9.1" data-path="topics-covered-7.html"><a href="topics-covered-7.html"><i class="fa fa-check"></i><b>9.1</b> Topics Covered</a></li>
<li class="chapter" data-level="9.2" data-path="clustering-on-geographic-coordinates.html"><a href="clustering-on-geographic-coordinates.html"><i class="fa fa-check"></i><b>9.2</b> Clustering on Geographic Coordinates</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="clustering-on-geographic-coordinates.html"><a href="clustering-on-geographic-coordinates.html#implementation-7"><i class="fa fa-check"></i><b>9.2.1</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="including-geographical-coordinates-in-the-feature-set.html"><a href="including-geographical-coordinates-in-the-feature-set.html"><i class="fa fa-check"></i><b>9.3</b> Including Geographical Coordinates in the Feature Set</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="including-geographical-coordinates-in-the-feature-set.html"><a href="including-geographical-coordinates-in-the-feature-set.html#implementation-8"><i class="fa fa-check"></i><b>9.3.1</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="spatialweightedcluster.html"><a href="spatialweightedcluster.html"><i class="fa fa-check"></i><b>9.4</b> Weighted Optimization of Geographical and Attribute Similarity</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="spatialweightedcluster.html"><a href="spatialweightedcluster.html#optimization"><i class="fa fa-check"></i><b>9.4.1</b> Optimization</a>
<ul>
<li class="chapter" data-level="9.4.1.1" data-path="spatialweightedcluster.html"><a href="spatialweightedcluster.html#connectivity-check"><i class="fa fa-check"></i><b>9.4.1.1</b> Connectivity check</a></li>
</ul></li>
<li class="chapter" data-level="9.4.2" data-path="spatialweightedcluster.html"><a href="spatialweightedcluster.html#implementation-9"><i class="fa fa-check"></i><b>9.4.2</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="constructingspatialcontiguous.html"><a href="constructingspatialcontiguous.html"><i class="fa fa-check"></i><b>9.5</b> Constructing a Spatially Contiguous Solution</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="constructingspatialcontiguous.html"><a href="constructingspatialcontiguous.html#implementation-10"><i class="fa fa-check"></i><b>9.5.1</b> Implementation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="CHspatialhierarchical.html"><a href="CHspatialhierarchical.html"><i class="fa fa-check"></i><b>10</b> Spatially Constrained Clustering - Hierarchical Methods</a>
<ul>
<li class="chapter" data-level="10.1" data-path="topics-covered-8.html"><a href="topics-covered-8.html"><i class="fa fa-check"></i><b>10.1</b> Topics Covered</a></li>
<li class="chapter" data-level="10.2" data-path="spatially-constrained-hierarchical-clustering-schc.html"><a href="spatially-constrained-hierarchical-clustering-schc.html"><i class="fa fa-check"></i><b>10.2</b> Spatially Constrained Hierarchical Clustering (SCHC)</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="spatially-constrained-hierarchical-clustering-schc.html"><a href="spatially-constrained-hierarchical-clustering-schc.html#schcalgillustration"><i class="fa fa-check"></i><b>10.2.1</b> The Algorithm</a>
<ul>
<li class="chapter" data-level="10.2.1.1" data-path="spatially-constrained-hierarchical-clustering-schc.html"><a href="spatially-constrained-hierarchical-clustering-schc.html#schccompletelinkage"><i class="fa fa-check"></i><b>10.2.1.1</b> SCHC Complete Linkage</a></li>
</ul></li>
<li class="chapter" data-level="10.2.2" data-path="spatially-constrained-hierarchical-clustering-schc.html"><a href="spatially-constrained-hierarchical-clustering-schc.html#schcimplement"><i class="fa fa-check"></i><b>10.2.2</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="skater.html"><a href="skater.html"><i class="fa fa-check"></i><b>10.3</b> SKATER</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="skater.html"><a href="skater.html#skaterpruning"><i class="fa fa-check"></i><b>10.3.1</b> Pruning the Minimum Spanning Tree</a></li>
<li class="chapter" data-level="10.3.2" data-path="skater.html"><a href="skater.html#implementation-11"><i class="fa fa-check"></i><b>10.3.2</b> Implementation</a>
<ul>
<li class="chapter" data-level="10.3.2.1" data-path="skater.html"><a href="skater.html#saveMST"><i class="fa fa-check"></i><b>10.3.2.1</b> Saving the Minimum Spanning Tree</a></li>
<li class="chapter" data-level="10.3.2.2" data-path="skater.html"><a href="skater.html#setskaterminsize"><i class="fa fa-check"></i><b>10.3.2.2</b> Setting a minimum cluster size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="redcap.html"><a href="redcap.html"><i class="fa fa-check"></i><b>10.4</b> REDCAP</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="redcap.html"><a href="redcap.html#illustration---fullorder-completelinkage"><i class="fa fa-check"></i><b>10.4.1</b> Illustration - FullOrder-CompleteLinkage</a></li>
<li class="chapter" data-level="10.4.2" data-path="redcap.html"><a href="redcap.html#redcapimplementation"><i class="fa fa-check"></i><b>10.4.2</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i><b>10.5</b> Assessment</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="CHspatialpartition.html"><a href="CHspatialpartition.html"><i class="fa fa-check"></i><b>11</b> Spatially Constrained Clustering - Partitioning Methods</a>
<ul>
<li class="chapter" data-level="11.1" data-path="topics-covered-9.html"><a href="topics-covered-9.html"><i class="fa fa-check"></i><b>11.1</b> Topics Covered</a></li>
<li class="chapter" data-level="11.2" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html"><i class="fa fa-check"></i><b>11.2</b> Automatic Zoning Procedure (AZP)</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#azp-heuristic"><i class="fa fa-check"></i><b>11.2.1</b> AZP Heuristic</a>
<ul>
<li class="chapter" data-level="11.2.1.1" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#illustration"><i class="fa fa-check"></i><b>11.2.1.1</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="11.2.2" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#tabu-search"><i class="fa fa-check"></i><b>11.2.2</b> Tabu Search</a></li>
<li class="chapter" data-level="11.2.3" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#simulated-annealing"><i class="fa fa-check"></i><b>11.2.3</b> Simulated Annealing</a></li>
<li class="chapter" data-level="11.2.4" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#arisel"><i class="fa fa-check"></i><b>11.2.4</b> ARiSeL</a></li>
<li class="chapter" data-level="11.2.5" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#using-the-outcome-from-another-cluster-routine-as-the-initial-feasible-region"><i class="fa fa-check"></i><b>11.2.5</b> Using the Outcome from Another Cluster Routine as the Initial Feasible Region</a></li>
<li class="chapter" data-level="11.2.6" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#implementation-12"><i class="fa fa-check"></i><b>11.2.6</b> Implementation</a></li>
<li class="chapter" data-level="11.2.7" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#search-options"><i class="fa fa-check"></i><b>11.2.7</b> Search Options</a>
<ul>
<li class="chapter" data-level="11.2.7.1" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#local-search"><i class="fa fa-check"></i><b>11.2.7.1</b> Local Search</a></li>
<li class="chapter" data-level="11.2.7.2" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#tabu-search-1"><i class="fa fa-check"></i><b>11.2.7.2</b> Tabu search</a></li>
<li class="chapter" data-level="11.2.7.3" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#simulated-annealing-1"><i class="fa fa-check"></i><b>11.2.7.3</b> Simulated annealing</a></li>
</ul></li>
<li class="chapter" data-level="11.2.8" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#initialization-options"><i class="fa fa-check"></i><b>11.2.8</b> Initialization Options</a>
<ul>
<li class="chapter" data-level="11.2.8.1" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#arisel-1"><i class="fa fa-check"></i><b>11.2.8.1</b> ARiSeL</a></li>
<li class="chapter" data-level="11.2.8.2" data-path="automatic-zoning-procedure-azp.html"><a href="automatic-zoning-procedure-azp.html#initial-regions"><i class="fa fa-check"></i><b>11.2.8.2</b> Initial regions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="max-p-region-problem.html"><a href="max-p-region-problem.html"><i class="fa fa-check"></i><b>11.3</b> Max-P Region Problem</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="max-p-region-problem.html"><a href="max-p-region-problem.html#max-p-heuristic"><i class="fa fa-check"></i><b>11.3.1</b> Max-p Heuristic</a>
<ul>
<li class="chapter" data-level="11.3.1.1" data-path="max-p-region-problem.html"><a href="max-p-region-problem.html#illustration-1"><i class="fa fa-check"></i><b>11.3.1.1</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="11.3.2" data-path="max-p-region-problem.html"><a href="max-p-region-problem.html#implementation-13"><i class="fa fa-check"></i><b>11.3.2</b> Implementation</a></li>
<li class="chapter" data-level="11.3.3" data-path="max-p-region-problem.html"><a href="max-p-region-problem.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>11.3.3</b> Sensitivity Analysis</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Assessment</b></span></li>
<li class="chapter" data-level="12" data-path="CHclustervalidation.html"><a href="CHclustervalidation.html"><i class="fa fa-check"></i><b>12</b> Cluster Validation</a>
<ul>
<li class="chapter" data-level="12.1" data-path="topics-covered-10.html"><a href="topics-covered-10.html"><i class="fa fa-check"></i><b>12.1</b> Topics Covered</a></li>
<li class="chapter" data-level="12.2" data-path="internal-validity.html"><a href="internal-validity.html"><i class="fa fa-check"></i><b>12.2</b> Internal Validity</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="internal-validity.html"><a href="internal-validity.html#traditional-measures-of-fit"><i class="fa fa-check"></i><b>12.2.1</b> Traditional Measures of Fit</a></li>
<li class="chapter" data-level="12.2.2" data-path="internal-validity.html"><a href="internal-validity.html#clusterbalance"><i class="fa fa-check"></i><b>12.2.2</b> Balance</a></li>
<li class="chapter" data-level="12.2.3" data-path="internal-validity.html"><a href="internal-validity.html#join-count-ratio"><i class="fa fa-check"></i><b>12.2.3</b> Join Count Ratio</a></li>
<li class="chapter" data-level="12.2.4" data-path="internal-validity.html"><a href="internal-validity.html#compactness"><i class="fa fa-check"></i><b>12.2.4</b> Compactness</a></li>
<li class="chapter" data-level="12.2.5" data-path="internal-validity.html"><a href="internal-validity.html#connectedness"><i class="fa fa-check"></i><b>12.2.5</b> Connectedness</a></li>
<li class="chapter" data-level="12.2.6" data-path="internal-validity.html"><a href="internal-validity.html#implementation-14"><i class="fa fa-check"></i><b>12.2.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="external-validity.html"><a href="external-validity.html"><i class="fa fa-check"></i><b>12.3</b> External Validity</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="external-validity.html"><a href="external-validity.html#classic-measures"><i class="fa fa-check"></i><b>12.3.1</b> Classic Measures</a>
<ul>
<li class="chapter" data-level="12.3.1.1" data-path="external-validity.html"><a href="external-validity.html#adjusted-rand-index"><i class="fa fa-check"></i><b>12.3.1.1</b> Adjusted Rand Index</a></li>
<li class="chapter" data-level="12.3.1.2" data-path="external-validity.html"><a href="external-validity.html#normalized-information-distance"><i class="fa fa-check"></i><b>12.3.1.2</b> Normalized Information Distance</a></li>
</ul></li>
<li class="chapter" data-level="12.3.2" data-path="external-validity.html"><a href="external-validity.html#visualizing-cluster-match"><i class="fa fa-check"></i><b>12.3.2</b> Visualizing Cluster Match</a>
<ul>
<li class="chapter" data-level="12.3.2.1" data-path="external-validity.html"><a href="external-validity.html#linking-cluster-maps"><i class="fa fa-check"></i><b>12.3.2.1</b> Linking Cluster Maps</a></li>
<li class="chapter" data-level="12.3.2.2" data-path="external-validity.html"><a href="external-validity.html#cluster-match-map"><i class="fa fa-check"></i><b>12.3.2.2</b> Cluster Match Map</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="beyond-clustering.html"><a href="beyond-clustering.html"><i class="fa fa-check"></i><b>12.4</b> Beyond Clustering</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
<li> Copyright (c) 2023, Luc Anselin</li>
<li> All Rights Reserved</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Spatial Data Science with GeoDa</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="principal-components" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Principal Components<a href="principal-components.html#principal-components" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Principal components analysis has an eminent historical pedigree, going back to pioneering work in the early twentieth century by the statistician Karl Pearson <span class="citation">(<a href="references.html#ref-Pearson:01">Pearson 1901</a>)</span> and the economist Harold Hotelling <span class="citation">(<a href="references.html#ref-Hotelling:33">Hotelling 1933</a>)</span>. The technique is also known as the
Karhunen-Loève transform in probability theory, and as empirical orthogonal functions or EOF in meteorology <span class="citation">(see, for example, in applications of space-time statistics in <a href="references.html#ref-CressieWikle:11">Cressie and Wikle 2011</a>; <a href="references.html#ref-Wikleetal:19">Wikle, Zammit-Mangion, and Cressie 2019</a>)</span>.</p>
<p>The derivation of principal components can be approached from a number of different perspectives, all leading to the same solution. Perhaps the most common treatment considers the components as the solution of a problem of finding
new variables that are constructed as a linear combination of the
original variables, such that they maximize the explained variance.
In a sense, the
principal components can be interpreted as the best linear approximation to the multivariate point cloud of the data.</p>
<p>The point of departure is to organize <span class="math inline">\(n\)</span> observations on <span class="math inline">\(k\)</span> variables <span class="math inline">\(x_h\)</span>,
with <span class="math inline">\(h = 1, \dots, k\)</span>,
as a <span class="math inline">\(n \times k\)</span> matrix <span class="math inline">\(X\)</span> (each variable is a column in the matrix). In practice, each of the variables is typically standardized, such that its mean is zero and variance equals one. This avoids problems with (large) scale differences between the variables (i.e., some are very small numbers and others very large). For such standardized variables, the <span class="math inline">\(k \times k\)</span> cross product matrix <span class="math inline">\(X&#39;X\)</span> corresponds to the correlation matrix (without standardization, this would be the variance-covariance matrix).<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>The goal is to find a small number of new variables, the so-called <em>principal components</em>, that explain the bulk of the variance (or, in practice, the correlation) in the original variables. If this can be accomplished with a much smaller number of variables than in the original set, the objective of <em>dimension reduction</em> will have been achieved.</p>
<p>Each principal component <span class="math inline">\(z_u\)</span> is a linear combination of the original variables <span class="math inline">\(x_h\)</span>, with <span class="math inline">\(h\)</span> going from <span class="math inline">\(1\)</span> to <span class="math inline">\(k\)</span> such that:
<span class="math display">\[z_u = a_1 x_1 + a_2 x_2 + \dots + a_k x_k\]</span>
The mathematical problem is to find the coefficients <span class="math inline">\(a_h\)</span> such that the new variables maximize the explained variance of the original variables. In addition, to avoid an indeterminate solution, the coefficients are scaled
such that the sum of their squares equals <span class="math inline">\(1\)</span>.</p>
<p>A full mathematical treatment of the derivation of the optimal solution to this problem is beyond the current scope <span class="citation">(for details, see, e.g., <a href="references.html#ref-LeeVerleysen:07">Lee and Verleysen 2007, chap. 2</a>)</span>. Nevertheless, obtaining a basic intuition for the mathematical principles involved is useful.</p>
<p>The coefficients by which the original variables need to be
multiplied to obtain each principal component can be shown to correspond to
the elements of the eigenvectors
of <span class="math inline">\(X&#39;X\)</span>, with the associated eigenvalue giving the explained variance. Even though the original data matrix <span class="math inline">\(X\)</span> is typically not square (of dimension <span class="math inline">\(n \times k\)</span>), the cross-product matrix <span class="math inline">\(X&#39;X\)</span> is of dimension <span class="math inline">\(k \times k\)</span>, so it is square and symmetric. As a result, all the eigenvalues are real numbers, which avoids having to deal with complex numbers.</p>
<p>Operationally, the principal component coefficients are obtained by means of a
matrix decomposition. One option is to compute the <em>spectral</em> decomposition of the
<span class="math inline">\(k \times k\)</span> matrix <span class="math inline">\(X&#39;X\)</span>, i.e., of the correlation matrix. As shown in Section <a href="matrix-algebra-review.html#spectraldecomposition">2.2.2.1</a>, this yields:
<span class="math display">\[X&#39;X = VGV&#39;,\]</span>
where <span class="math inline">\(V\)</span> is a <span class="math inline">\(k \times k\)</span> matrix with the eigenvectors as columns (the coefficients needed
to construct the principal components), and <span class="math inline">\(G\)</span> a <span class="math inline">\(k \times k\)</span> diagonal matrix of the
associated eigenvalues (the explained variance).</p>
<p>A principal component for each observation is obtained by multiplying the row of standardized observations by the
column of eigenvalues, i.e., a column of the matrix <span class="math inline">\(V\)</span>. More formally, all the principal components are obtained concisely as:
<span class="math display">\[XV.\]</span></p>
<p>A second, and computationally preferred way to approach this is as a <em>singular value decomposition</em> (SVD)
of the <span class="math inline">\(n \times k\)</span> matrix <span class="math inline">\(X\)</span>, i.e., the matrix of (standardized) observations. From Section <a href="matrix-algebra-review.html#svd">2.2.2.2</a>, this follows as
<span class="math display">\[X = UDV&#39;,\]</span>
where again <span class="math inline">\(V\)</span> (the transpose of the <span class="math inline">\(k \times k\)</span> matrix <span class="math inline">\(V&#39;\)</span>) is the matrix with the eigenvectors of <span class="math inline">\(X&#39;X\)</span> as columns, and <span class="math inline">\(D\)</span> is a <span class="math inline">\(k \times k\)</span>
diagonal matrix, containing the square root of the eigenvalues of <span class="math inline">\(X&#39;X\)</span> on the diagonal.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Note that the number of eigenvalues used in the spectral decomposition and in SVD is the same, and equals <span class="math inline">\(k\)</span>, the column dimension of <span class="math inline">\(X\)</span>.</p>
<p>Since <span class="math inline">\(V&#39;V = I\)</span>, the following result obtains when both sides of the SVD decomposition are post-multiplied by <span class="math inline">\(V\)</span>:
<span class="math display">\[XV = UDV&#39;V = UD.\]</span>
In other words, the principal components <span class="math inline">\(XV\)</span> can also be obtained as the product of the orthonormal matrix <span class="math inline">\(U\)</span> with a diagonal matrix containing the square root of the eigenvalues, <span class="math inline">\(D\)</span>. This result is important in the context of multidimensional scaling, considered in Chapter <a href="CHMDS.html#CHMDS">3</a>.</p>
<p>It turns out that the SVD approach is the solution to viewing the principal components explicitly as a dimension reduction problem, originally considered by Karl Pearson. The observed vector on the <span class="math inline">\(k\)</span> variables <span class="math inline">\(x\)</span> can be expressed as a function of a number of unknown <em>latent variables</em> <span class="math inline">\(z\)</span>, such that there is a linear relationship between them:
<span class="math display">\[ x = Az, \]</span>
where <span class="math inline">\(x\)</span> is a <span class="math inline">\(k \times 1\)</span> vector of the observed variables, and <span class="math inline">\(z\)</span> is a <span class="math inline">\(p \times 1\)</span> vector of the (unobserved) latent variables, ideally with <span class="math inline">\(p\)</span> much smaller than <span class="math inline">\(k\)</span>. The matrix <span class="math inline">\(A\)</span> is of dimension <span class="math inline">\(k \times p\)</span> and contains the coefficients of the transformation. Again, in order to avoid indeterminate solutions, the coefficients are scaled such that <span class="math inline">\(A&#39;A = I\)</span>, which
ensures that the sum of squares of the coefficients associated with a given component equals one.</p>
<p>Instead of maximizing explained variance, the objective is now to find <span class="math inline">\(A\)</span> and <span class="math inline">\(z\)</span> such that the so-called reconstruction error is minimized.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>Importantly, different computational approaches to obtain the eigenvalues and eigenvectors (there is no analytical solution) may yield opposite signs for the elements of the eigenvectors. However, the eigenvalues will be the same. The sign of the eigenvectors will affect the sign of the resulting component, i.e., positives become negatives. For example, this can be the difference between results based on a spectral decomposition versus SVD.</p>
<p>In a principal component analysis, the interest typically focuses on three main results. First, the principal component scores are used as a replacement for the original variables. This is particularly relevant when a small number of components explain a substantial share of the original variance. Second,
the
relative contribution of each of the original variables to each principal component is of interest. Finally, the
variance proportion explained by each component in and of itself is also important.</p>
<div id="implementation" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Implementation<a href="principal-components.html#implementation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Principal components are invoked from the drop down list created by the toolbar <strong>Clusters</strong> icon (Figure <a href="topics-covered.html#fig:pcaicon">2.1</a>) as the top item (more precisely, the first item in the dimension reduction category). Alternatively, from the main menu, <strong>Clusters &gt; PCA</strong> gets the process started.</p>
<p>The illustration uses ten variables that characterize the efficiency of community banks, based on the observations for 2013 from the <em>Italy Community Bank</em> sample data set <span class="citation">(see <a href="references.html#ref-Algerietal:22">Algeri et al. 2022</a>)</span>:</p>
<ul>
<li>CAPRAT: ratio of capital over risk weighted assets</li>
<li>Z: z score of return on assets (ROA) + leverage over the standard deviation of ROA</li>
<li>LIQASS: ratio of liquid assets over total assets</li>
<li>NPL: ratio of non-performing loans over total loans</li>
<li>LLP: ratio of loan loss provision over customer loans</li>
<li>INTR: ratio of interest expense over total funds</li>
<li>DEPO: ratio of total deposits over total assets</li>
<li>EQLN: ratio of total equity over customer loans</li>
<li>SERV: ratio of net interest income over total operating revenues</li>
<li>EXPE: ratio of operating expenses over total assets</li>
</ul>
<p>Some descriptive statistics are listed in Figure <a href="principal-components.html#fig:italystats">2.3</a>. An analysis of individual box plots (not shown) reveals that most distributions are quite skewed, with only NPL, INTR and DEPO not showing any outliers. SERV is the only variable with outliers at the low end of the distribution. All the other variables have a considerable number of outlying observations at the high end <span class="citation">(see <a href="references.html#ref-Algerietal:22">Algeri et al. 2022</a> for a substantive discussion of the variables)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:italystats"></span>
<img src="pics/pics20/20_13_descriptive_stats.png" alt="Italy Bank Characteristics Descriptive Statistics" width="50%" />
<p class="caption">
Figure 2.3: Italy Bank Characteristics Descriptive Statistics
</p>
</div>
<p>The correlation matrix (not shown) includes both very strong linear relations between pairs
of variables as well as very low ones. For example, NPL is highly correlated with SERV (-0.90) and LLP (0.64),
as is CAPRATA with EQLN (0.87), but LIQASS is essentially uncorrelated with NPL (-0.004) and SERV (0.01).</p>
<p>Selection of PCA brings up the <strong>PCA Settings Menu</strong>, which is the main interface to specify all the settings.
This interface has a similar structure for all multivariate analyses and is shown in Figure <a href="principal-components.html#fig:pcasettings">2.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcasettings"></span>
<img src="pics/pics20/20_02_pca_settings.png" alt="PCA Settings Menu" width="35%" />
<p class="caption">
Figure 2.4: PCA Settings Menu
</p>
</div>
<p>The top dialog is the interface to <strong>Select Variables</strong>. The default <strong>Method</strong> to compute the various coefficients is <strong>SVD</strong>. The other option is <strong>Eigen</strong>, which uses spectral decomposition. By default, all variables are used as <strong>Standardize (Z)</strong>, such that the mean is zero and the
standard deviation is one.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>The <strong>Run</strong> button computes the principal components and brings up the results in the right-hand panel, as shown in Figure <a href="principal-components.html#fig:pcaresults">2.5</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcaresults"></span>
<img src="pics/pics20/20_03_pca_results.png" alt="PCA Results" width="100%" />
<p class="caption">
Figure 2.5: PCA Results
</p>
</div>
<p>The result summary is evaluated in more detail in Section <a href="principal-components.html#pcainterpretation">2.3.2</a>.</p>
<div id="saving-the-principal-components" class="section level4 hasAnchor" number="2.3.1.1">
<h4><span class="header-section-number">2.3.1.1</span> Saving the principal components<a href="principal-components.html#saving-the-principal-components" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Once the computation is finished, the resulting principal components become available to be added to the Data Table
as new variables. The <strong>Components</strong> drop-down list suggests the number of components based on the 95% variance criterion (see Section <a href="principal-components.html#pcainterpretation">2.3.2</a>). In the example, this is <strong>7</strong>.</p>
<p>Invoking the <strong>Save</strong> button brings up a dialog to specify the variable names for the principal components, with as default
<strong>PC1</strong>, <strong>PC2</strong>, etc. These variables are added to the Data Table and become available for any subsequent analysis or visualization.
This is illustrated in Figure <a href="principal-components.html#fig:pcasave">2.6</a> for seven components based on the ten original bank variables.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcasave"></span>
<img src="pics/pics20/20_04_pca_intable.png" alt="Principal Components in the Data Table" width="60%" />
<p class="caption">
Figure 2.6: Principal Components in the Data Table
</p>
</div>
</div>
<div id="saving-the-result-summary" class="section level4 hasAnchor" number="2.3.1.2">
<h4><span class="header-section-number">2.3.1.2</span> Saving the result summary<a href="principal-components.html#saving-the-result-summary" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The listing of the PCA results, including eigenvalues, loadings and squared correlations can be saved to a text file by a right click in the window and selecting <strong>Save</strong>. The resulting text file is an exact replica of the result listing.</p>
</div>
</div>
<div id="pcainterpretation" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Interpretation<a href="principal-components.html#pcainterpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The panel with summary results (Figure <a href="principal-components.html#fig:pcaresults">2.5</a>) provides several statistics pertaining to the variance decomposition, the eigenvalues,
the variable loadings and the contribution of each of the original variables to the
respective components.</p>
<div id="pcaexplainedvar" class="section level4 hasAnchor" number="2.3.2.1">
<h4><span class="header-section-number">2.3.2.1</span> Explained variance<a href="principal-components.html#pcaexplainedvar" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>After listing the <strong>PCA method</strong> (here the default <strong>SVD</strong>), the first item in the results panel gives the <strong>Standard deviation</strong> explained by each of the components. It corresponds to the
square root of the <strong>Eigenvalues</strong> (each eigenvalue equals the variance explained
by the corresponding principal component), which are listed as well. In the example, the first eigenvalue is 2.98675, which is thus the variance of the first component. Consequently, the standard deviation is the square root of this value, i.e., 1.728221, given as the first item in the list.</p>
<p>The sum of all the eigenvalues is 10, which equals the number of variables, or, more precisely, the rank of the matrix <span class="math inline">\(X&#39;X\)</span>. Therefore, the proportion of variance explained by the first component is 2.98675/10 = 0.2987, as reported in the list. Similarly, the proportion explained by the second component is 0.1763, so that the cumulative proportion of the first and second component amounts to 0.2987 + 0.1763 = 0.4750. In other words, the first two components explain a little less than half of the total variance.</p>
<p>The fraction of the total variance explained is listed both as a separate
<strong>Proportion</strong> and as a <strong>Cumulative proportion</strong>. The latter is typically used to choose a
cut-off for the number of components. A common convention is to take a threshold of 95%,
which would suggest 7 components in the
example.</p>
<p>An alternative criterion to select the number of components is the so-called
<strong>Kaiser</strong> criterion <span class="citation">(<a href="references.html#ref-Kaiser:60">Kaiser 1960</a>)</span>, which suggests to take the components for which the eigenvalue
exceeds <strong>1</strong>. In the example, this would yield 3 components (they explain slightly more than 60% of the total variance).</p>
<p>The bottom part of the results panel is occupied by two tables that have the
original variables as rows and the components as columns.</p>
</div>
<div id="variable-loadings" class="section level4 hasAnchor" number="2.3.2.2">
<h4><span class="header-section-number">2.3.2.2</span> Variable loadings<a href="principal-components.html#variable-loadings" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The first table that relates principal components to the original variables shows the
<strong>Variable Loadings</strong>. For each principal component (column), this lists the elements
of the corresponding eigenvector. The eigenvectors are standardized such that the sum
of the squared coefficients equals one. The elements of the eigenvector are the coefficients by which the (standardized) original
variables need to be multiplied to construct each component (see Section <a href="principal-components.html#loadingsandpca">2.3.2.3</a>).</p>
<p>It is important to keep in mind that the signs of the loadings may switch, depending on the algorithm that is used in the computation. However, the absolute value of the coefficients
remains the same. In the example, setting <strong>Method</strong> to <strong>Eigen</strong> yields the loadings shown in Figure <a href="principal-components.html#fig:pcaeigen">2.7</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcaeigen"></span>
<img src="pics/pics20/20_05_pca_eigen.png" alt="Variable Loadings Using the EIGEN Mehtod" width="100%" />
<p class="caption">
Figure 2.7: Variable Loadings Using the EIGEN Mehtod
</p>
</div>
<p>For PC2, PC6, PC7, and PC9, the signs for the loadings are the opposite of those reported in Figure <a href="principal-components.html#fig:pcaresults">2.5</a>.
This needs to be kept in mind when interpreting the actual value (and sign) of the components and when using the components as variables in subsequent analyses (e.g., regression) and visualization (see Section <a href="vizpca.html#vizpca">2.4</a>).</p>
<p>When the original variables are all standardized, each eigenvector coefficient
gives a measure of the relative contribution of a variable to the component in question.</p>
</div>
<div id="loadingsandpca" class="section level4 hasAnchor" number="2.3.2.3">
<h4><span class="header-section-number">2.3.2.3</span> Variable loadings and principal components<a href="principal-components.html#loadingsandpca" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The detailed computation of the principal components is illustrated for the first two observations on the first
component in Figure <a href="principal-components.html#fig:pcacalc">2.8</a>. The names of the standardized variables are listed in the left-most column, followed
by the principal component loadings (the values in the PC1 column in Figure <a href="principal-components.html#fig:pcaresults">2.5</a>). The next column shows the
standardized values for each variable for the first observation. These are multiplied by the loading in column four, with the
sum listed at the bottom. The value of 2.2624 matches the entry in the first row under PC1 in Figure <a href="principal-components.html#fig:pcasave">2.6</a>.</p>
<p>Similarly, the value of 1.1769 for the second row under PC1 is obtained at the bottom of column 6. Similar calculations can be carried out to verify the other entries.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcacalc"></span>
<img src="pics/pics20/20_06_pc_calculation.png" alt="Principal Component Calculation" width="60%" />
<p class="caption">
Figure 2.8: Principal Component Calculation
</p>
</div>
</div>
<div id="substantive-interpretation---squared-correlation" class="section level4 hasAnchor" number="2.3.2.4">
<h4><span class="header-section-number">2.3.2.4</span> Substantive interpretation - squared correlation<a href="principal-components.html#substantive-interpretation---squared-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The interpretation and substantive meaning of the principal components can
be a challenge. In <em>factor analysis</em>, a number of rotations are applied to clarify the contribution of each variable to the different components. The latter are then imbued with meaning such as “social deprivation”, “religious climate”, etc. Principal component analysis tends to stay away from this, but nevertheless, it is useful to consider the relative contribution of each variable to the respective components.</p>
<p>The table labeled as <strong>Squared correlations</strong> lists those statistics between each of the original
variables in a row and the principal component listed in the column. Each row of the table shows how much of the variance
in the original variable is explained by each of the components. As a result, the values
in each row sum to one.</p>
<p>More insightful is the analysis of each column, which indicates
which variables are important in the computation of the matching component. In the example, <strong>INTR</strong> (61.7%), <strong>EQLN</strong> (57.3%) and
<strong>CAPRAT</strong> (51.8%) dominate the contributions to the first principal component. In the second component, the main contributor is
<strong>NPL</strong> (52.6%), as well as <strong>Z</strong> (31.1%). This provides a way to interpret how the multiple dimensions along the ten original variables are summarized into the main principal components.</p>
<p>Since the correlations are squared, they do not depend on the sign of the eigenvector elements, unlike the loadings.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>The standardization should not be done mechanically, since there are instances where the variance differences between the variables are actually meaningful, e.g., when the scales on which they are measured have a strong substantive meaning (e.g., in psychology).<a href="principal-components.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Since the eigenvalues equal the variance explained by the corresponding component, the diagonal elements of <span class="math inline">\(D\)</span> are thus the standard deviation explained by the component.<a href="principal-components.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>The concept of reconstruction error is somewhat technical. If <span class="math inline">\(A\)</span> were a square matrix, one could solve for <span class="math inline">\(z\)</span> as
<span class="math inline">\(z = A^{-1}x\)</span>, where <span class="math inline">\(A^{-1}\)</span> is the inverse of the matrix <span class="math inline">\(A\)</span>. However, due to the dimension reduction, <span class="math inline">\(A\)</span> is not square, so something called a pseudo-inverse or Moore-Penrose inverse must be used. This is the <span class="math inline">\(p \times k\)</span> matrix
<span class="math inline">\((A&#39;A)^{-1}A&#39;\)</span>, such that <span class="math inline">\(z = (A&#39;A)^{-1}A&#39;x\)</span>. Furthermore, because <span class="math inline">\(A&#39;A = I\)</span>, this simplifies to <span class="math inline">\(z = A&#39;x\)</span> (of course, so far the elements of <span class="math inline">\(A\)</span> are unknown). Since <span class="math inline">\(x = Az\)</span>, if <span class="math inline">\(A\)</span> were known, <span class="math inline">\(x\)</span> could be found as <span class="math inline">\(Az\)</span>, or, as <span class="math inline">\(AA&#39;x\)</span>. The reconstruction error is then the squared difference between <span class="math inline">\(x\)</span> and <span class="math inline">\(AA&#39;x\)</span>. The objective is to find the coefficients for <span class="math inline">\(A\)</span> that minimize this expression. For an extensive technical discussion, see <span class="citation">Lee and Verleysen (<a href="references.html#ref-LeeVerleysen:07">2007</a>)</span>, Chapter 2.<a href="principal-components.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>A full list of the standardization options in <code>GeoDa</code> is given in Chapter 2 of Volume 1.<a href="principal-components.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="matrix-algebra-review.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="vizpca.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yihui/bookdown-crc/edit/master/20.PCA.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
